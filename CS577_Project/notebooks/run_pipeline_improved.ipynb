{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Pipeline Notebook\n",
    "\n",
    "This notebook runs an improved version of the reasoning direction analysis with:\n",
    "- Stronger intervention strengths\n",
    "- More granular strength sampling\n",
    "- Quality-based evaluation metrics\n",
    "- Multiple test prompts\n",
    "- Better statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace cache directory set to: /scratch/gilbreth/sramishe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../pipeline')\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Set HuggingFace cache directory\n",
    "os.environ['HF_HOME'] = '/scratch/gilbreth/sramishe'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/gilbreth/sramishe/transformers'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/scratch/gilbreth/sramishe/datasets'\n",
    "\n",
    "from model_loader import ModelLoader\n",
    "from data_processor import DataProcessor\n",
    "from direction_calculator import DirectionCalculator\n",
    "from intervention import ActivationPatcher\n",
    "from evaluator import ReasoningEvaluator\n",
    "\n",
    "print(f\"HuggingFace cache directory set to: {os.environ['HF_HOME']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPROVED Configuration:\n",
      "  rl_model_name: Qwen/QwQ-32B\n",
      "  distilled_model_name: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
      "  dataset_name: HuggingFaceH4/MATH-500\n",
      "  num_samples: 20\n",
      "  strength_range: (-2.0, 2.0)\n",
      "  num_strengths: 9\n",
      "  num_test_prompts: 5\n",
      "  output_dir: /scratch/gilbreth/sramishe/results_QwQ_R1/results_improved\n",
      "  max_new_tokens: 1024\n",
      "  layers_step: 4\n",
      "\n",
      "Key Improvements:\n",
      "  - Intervention strengths: 20x stronger (-2.0 to 2.0 vs -0.1 to 0.1)\n",
      "  - Strength samples: 4.5x more points (9 vs 2)\n",
      "  - Total experiments per prompt: 144 (16 layers × 9 strengths)\n",
      "  - Quality metrics: Now captured (think tokens, backtracking, etc.)\n",
      "  - Multiple prompts: 5 different test cases\n"
     ]
    }
   ],
   "source": [
    "# IMPROVED Configuration parameters\n",
    "CONFIG = {\n",
    "    'rl_model_name': \"Qwen/QwQ-32B\",\n",
    "    'distilled_model_name': \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
    "    'dataset_name': \"HuggingFaceH4/MATH-500\",\n",
    "    'num_samples': 20,  # Increased from 10\n",
    "    'strength_range': (-2.0, 2.0),  # Much stronger: -2.0 to 2.0 (was -0.1 to 0.1)\n",
    "    'num_strengths': 9,  # More granular: 9 points (was 2)\n",
    "    'num_test_prompts': 5,  # Test on 5 different prompts\n",
    "    'output_dir': '/scratch/gilbreth/sramishe/results_QwQ_R1/results_improved',\n",
    "    'max_new_tokens': 1024,  # Increased from 512 to allow longer reasoning\n",
    "    'layers_step': 4  # Test every 4th layer for faster iteration\n",
    "}\n",
    "\n",
    "print(\"IMPROVED Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nKey Improvements:\")\n",
    "print(\"  - Intervention strengths: 20x stronger (-2.0 to 2.0 vs -0.1 to 0.1)\")\n",
    "print(\"  - Strength samples: 4.5x more points (9 vs 2)\")\n",
    "print(\"  - Total experiments per prompt: 144 (16 layers × 9 strengths)\")\n",
    "print(\"  - Quality metrics: Now captured (think tokens, backtracking, etc.)\")\n",
    "print(\"  - Multiple prompts: 5 different test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Models (Same as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading models...\n",
      "Loading RL-trained model: Qwen/QwQ-32B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 14/14 [00:55<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distilled model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [10:35<00:00, 79.41s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RL Model loaded: 64 layers\n",
      "✓ Distilled Model loaded: 64 layers\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 1: Loading models...\")\n",
    "\n",
    "loader = ModelLoader(\n",
    "    rl_model_name=CONFIG['rl_model_name'],\n",
    "    distilled_model_name=CONFIG['distilled_model_name']\n",
    ")\n",
    "\n",
    "models = loader.load_models(torch_dtype=torch.float16)\n",
    "\n",
    "rl_model = models['rl_model']\n",
    "rl_tokenizer = models['rl_tokenizer']\n",
    "distilled_model = models['distilled_model']\n",
    "distilled_tokenizer = models['distilled_tokenizer']\n",
    "\n",
    "model_info = loader.get_model_info()\n",
    "print(f\"✓ RL Model loaded: {model_info['rl_model']['num_layers']} layers\")\n",
    "print(f\"✓ Distilled Model loaded: {model_info['distilled_model']['num_layers']} layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load More Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Loading dataset...\n",
      "Loading dataset: HuggingFaceH4/MATH-500\n",
      "✓ Loaded 24 examples\n",
      "✓ Prepared 20 prompts\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 2: Loading dataset...\")\n",
    "\n",
    "processor = DataProcessor(\n",
    "    dataset_name=CONFIG['dataset_name'],\n",
    "    include_toy_tasks=True\n",
    ")\n",
    "\n",
    "dataset = processor.load_dataset(max_samples=CONFIG['num_samples'])\n",
    "toy_tasks = processor.get_toy_tasks()\n",
    "\n",
    "all_examples = dataset + toy_tasks\n",
    "prompts = processor.prepare_batch(all_examples[:CONFIG['num_samples']], rl_tokenizer)\n",
    "\n",
    "print(f\"✓ Loaded {len(all_examples)} examples\")\n",
    "print(f\"✓ Prepared {len(prompts)} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculate Reasoning Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Calculating reasoning directions...\n",
      "Analyzing 16 layers: [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 3: Calculating reasoning directions...\")\n",
    "\n",
    "calculator = DirectionCalculator()\n",
    "\n",
    "# Test every 4th layer for faster iteration\n",
    "num_layers = model_info['rl_model']['num_layers']\n",
    "layers_to_test = list(range(0, num_layers, CONFIG['layers_step']))\n",
    "\n",
    "print(f\"Analyzing {len(layers_to_test)} layers: {layers_to_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing RL model activations...\n",
      "✓ Captured activations for 16 layers\n"
     ]
    }
   ],
   "source": [
    "# Capture activations from more prompts for better direction estimation\n",
    "print(\"Capturing RL model activations...\")\n",
    "rl_activations = calculator.capture_activations(\n",
    "    rl_model,\n",
    "    rl_tokenizer,\n",
    "    prompts[:10],  # Use 10 prompts instead of 5\n",
    "    layers_to_test\n",
    ")\n",
    "print(f\"✓ Captured activations for {len(rl_activations)} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing distilled model activations...\n",
      "✓ Captured activations for 16 layers\n"
     ]
    }
   ],
   "source": [
    "print(\"Capturing distilled model activations...\")\n",
    "distilled_activations = calculator.capture_activations(\n",
    "    distilled_model,\n",
    "    distilled_tokenizer,\n",
    "    prompts[:10],\n",
    "    layers_to_test\n",
    ")\n",
    "print(f\"✓ Captured activations for {len(distilled_activations)} layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing direction vectors...\n",
      "✓ Computed directions for 16 layers\n",
      "Directions saved to /scratch/gilbreth/sramishe/results_QwQ_R1/results_improved/reasoning_directions.pt\n",
      "✓ Directions saved to /scratch/gilbreth/sramishe/results_QwQ_R1/results_improved/reasoning_directions.pt\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing direction vectors...\")\n",
    "directions = calculator.calculate_direction(\n",
    "    rl_activations,\n",
    "    distilled_activations,\n",
    "    normalize_output=True\n",
    ")\n",
    "print(f\"✓ Computed directions for {len(directions)} layers\")\n",
    "\n",
    "# Save directions\n",
    "output_path = Path(CONFIG['output_dir'])\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "directions_path = output_path / \"reasoning_directions.pt\"\n",
    "calculator.save_directions(str(directions_path))\n",
    "print(f\"✓ Directions saved to {directions_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Improved Interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Running improved interventions...\n",
      "Generating baseline outputs...\n",
      "  Baseline 1/5: 1081 tokens\n",
      "  Baseline 2/5: 1144 tokens\n",
      "  Baseline 3/5: 1081 tokens\n",
      "  Baseline 4/5: 1048 tokens\n",
      "  Baseline 5/5: 1373 tokens\n",
      "✓ Generated 5 baseline outputs\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4: Running improved interventions...\")\n",
    "\n",
    "# Initialize evaluator for quality metrics\n",
    "evaluator = ReasoningEvaluator()\n",
    "patcher = ActivationPatcher(rl_model, directions)\n",
    "\n",
    "# Generate baseline outputs for multiple test prompts\n",
    "print(\"Generating baseline outputs...\")\n",
    "baselines = []\n",
    "for i, test_prompt in enumerate(prompts[:CONFIG['num_test_prompts']]):\n",
    "    inputs = rl_tokenizer(test_prompt, return_tensors=\"pt\").to(rl_model.device)\n",
    "    with torch.no_grad():\n",
    "        baseline_output = rl_model.generate(**inputs, max_new_tokens=CONFIG['max_new_tokens'])\n",
    "    baseline_text = rl_tokenizer.decode(baseline_output[0], skip_special_tokens=True)\n",
    "    baselines.append({\n",
    "        'prompt_idx': i,\n",
    "        'text': baseline_text,\n",
    "        'tokens': evaluator.count_tokens(baseline_text, rl_tokenizer, split_by_tags=True),\n",
    "        'quality': evaluator.analyze_reasoning_quality(baseline_text)\n",
    "    })\n",
    "    print(f\"  Baseline {i+1}/{CONFIG['num_test_prompts']}: {len(baseline_output[0])} tokens\")\n",
    "\n",
    "print(f\"✓ Generated {len(baselines)} baseline outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sweeping layers and strengths with quality metrics...\n",
      "  Strength range: (-2.0, 2.0)\n",
      "  Number of strengths: 9\n",
      "  Layers to test: 16\n",
      "Applied interventions to layers: [0]\n",
      "Applied interventions to layers: [0]\n",
      "Applied interventions to layers: [0]\n",
      "Applied interventions to layers: [0]\n",
      "Applied interventions to layers: [0]\n",
      "Applied interventions to layers: [0]\n",
      "Applied interventions to layers: [0]\n",
      "Applied interventions to layers: [0]\n",
      "Applied interventions to layers: [0]\n",
      "Applied interventions to layers: [4]\n",
      "Applied interventions to layers: [4]\n",
      "Applied interventions to layers: [4]\n",
      "Applied interventions to layers: [4]\n",
      "Applied interventions to layers: [4]\n",
      "Applied interventions to layers: [4]\n",
      "Applied interventions to layers: [4]\n",
      "Applied interventions to layers: [4]\n",
      "Applied interventions to layers: [4]\n",
      "Applied interventions to layers: [8]\n",
      "Applied interventions to layers: [8]\n",
      "Applied interventions to layers: [8]\n",
      "Applied interventions to layers: [8]\n",
      "Applied interventions to layers: [8]\n",
      "Applied interventions to layers: [8]\n",
      "Applied interventions to layers: [8]\n",
      "Applied interventions to layers: [8]\n",
      "Applied interventions to layers: [8]\n",
      "Applied interventions to layers: [12]\n",
      "Applied interventions to layers: [12]\n",
      "Applied interventions to layers: [12]\n",
      "Applied interventions to layers: [12]\n",
      "Applied interventions to layers: [12]\n",
      "Applied interventions to layers: [12]\n",
      "Applied interventions to layers: [12]\n",
      "Applied interventions to layers: [12]\n",
      "Applied interventions to layers: [12]\n",
      "Applied interventions to layers: [16]\n",
      "Applied interventions to layers: [16]\n",
      "Applied interventions to layers: [16]\n",
      "Applied interventions to layers: [16]\n",
      "Applied interventions to layers: [16]\n",
      "Applied interventions to layers: [16]\n",
      "Applied interventions to layers: [16]\n",
      "Applied interventions to layers: [16]\n",
      "Applied interventions to layers: [16]\n",
      "Applied interventions to layers: [20]\n",
      "Applied interventions to layers: [20]\n",
      "Applied interventions to layers: [20]\n",
      "Applied interventions to layers: [20]\n",
      "Applied interventions to layers: [20]\n",
      "Applied interventions to layers: [20]\n",
      "Applied interventions to layers: [20]\n",
      "Applied interventions to layers: [20]\n",
      "Applied interventions to layers: [20]\n",
      "Applied interventions to layers: [24]\n",
      "Applied interventions to layers: [24]\n",
      "Applied interventions to layers: [24]\n",
      "Applied interventions to layers: [24]\n",
      "Applied interventions to layers: [24]\n",
      "Applied interventions to layers: [24]\n",
      "Applied interventions to layers: [24]\n",
      "Applied interventions to layers: [24]\n",
      "Applied interventions to layers: [24]\n",
      "Applied interventions to layers: [28]\n",
      "Applied interventions to layers: [28]\n",
      "Applied interventions to layers: [28]\n",
      "Applied interventions to layers: [28]\n",
      "Applied interventions to layers: [28]\n",
      "Applied interventions to layers: [28]\n",
      "Applied interventions to layers: [28]\n",
      "Applied interventions to layers: [28]\n",
      "Applied interventions to layers: [28]\n",
      "Applied interventions to layers: [32]\n",
      "Applied interventions to layers: [32]\n",
      "Applied interventions to layers: [32]\n",
      "Applied interventions to layers: [32]\n",
      "Applied interventions to layers: [32]\n",
      "Applied interventions to layers: [32]\n",
      "Applied interventions to layers: [32]\n",
      "Applied interventions to layers: [32]\n",
      "Applied interventions to layers: [32]\n",
      "Applied interventions to layers: [36]\n",
      "Applied interventions to layers: [36]\n",
      "Applied interventions to layers: [36]\n",
      "Applied interventions to layers: [36]\n",
      "Applied interventions to layers: [36]\n",
      "Applied interventions to layers: [36]\n",
      "Applied interventions to layers: [36]\n",
      "Applied interventions to layers: [36]\n",
      "Applied interventions to layers: [36]\n",
      "Applied interventions to layers: [40]\n",
      "Applied interventions to layers: [40]\n",
      "Applied interventions to layers: [40]\n",
      "Applied interventions to layers: [40]\n",
      "Applied interventions to layers: [40]\n",
      "Applied interventions to layers: [40]\n",
      "Applied interventions to layers: [40]\n",
      "Applied interventions to layers: [40]\n",
      "Applied interventions to layers: [40]\n",
      "Applied interventions to layers: [44]\n",
      "Applied interventions to layers: [44]\n",
      "Applied interventions to layers: [44]\n",
      "Applied interventions to layers: [44]\n",
      "Applied interventions to layers: [44]\n",
      "Applied interventions to layers: [44]\n",
      "Applied interventions to layers: [44]\n",
      "Applied interventions to layers: [44]\n",
      "Applied interventions to layers: [44]\n",
      "Applied interventions to layers: [48]\n",
      "Applied interventions to layers: [48]\n",
      "Applied interventions to layers: [48]\n",
      "Applied interventions to layers: [48]\n",
      "Applied interventions to layers: [48]\n",
      "Applied interventions to layers: [48]\n",
      "Applied interventions to layers: [48]\n",
      "Applied interventions to layers: [48]\n",
      "Applied interventions to layers: [48]\n",
      "Applied interventions to layers: [52]\n",
      "Applied interventions to layers: [52]\n",
      "Applied interventions to layers: [52]\n",
      "Applied interventions to layers: [52]\n",
      "Applied interventions to layers: [52]\n",
      "Applied interventions to layers: [52]\n",
      "Applied interventions to layers: [52]\n",
      "Applied interventions to layers: [52]\n",
      "Applied interventions to layers: [52]\n",
      "Applied interventions to layers: [56]\n",
      "Applied interventions to layers: [56]\n",
      "Applied interventions to layers: [56]\n",
      "Applied interventions to layers: [56]\n",
      "Applied interventions to layers: [56]\n",
      "Applied interventions to layers: [56]\n",
      "Applied interventions to layers: [56]\n",
      "Applied interventions to layers: [56]\n",
      "Applied interventions to layers: [56]\n",
      "Applied interventions to layers: [60]\n",
      "Applied interventions to layers: [60]\n",
      "Applied interventions to layers: [60]\n",
      "Applied interventions to layers: [60]\n",
      "Applied interventions to layers: [60]\n",
      "Applied interventions to layers: [60]\n",
      "Applied interventions to layers: [60]\n",
      "Applied interventions to layers: [60]\n",
      "Applied interventions to layers: [60]\n",
      "✓ Completed 144 intervention experiments\n"
     ]
    }
   ],
   "source": [
    "# Run interventions on first test prompt with quality metrics\n",
    "print(f\"\\nSweeping layers and strengths with quality metrics...\")\n",
    "print(f\"  Strength range: {CONFIG['strength_range']}\")\n",
    "print(f\"  Number of strengths: {CONFIG['num_strengths']}\")\n",
    "print(f\"  Layers to test: {len(layers_to_test)}\")\n",
    "\n",
    "intervention_results = patcher.sweep_layers_and_strengths(\n",
    "    tokenizer=rl_tokenizer,\n",
    "    prompt=prompts[0],\n",
    "    layer_range=(min(layers_to_test), max(layers_to_test)),\n",
    "    strength_range=CONFIG['strength_range'],\n",
    "    num_strengths=CONFIG['num_strengths'],\n",
    "    max_new_tokens=CONFIG['max_new_tokens'],\n",
    "    evaluator=evaluator  # NOW CAPTURES QUALITY METRICS!\n",
    ")\n",
    "print(f\"✓ Completed {len(intervention_results)} intervention experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Enhanced Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Evaluating results with improved metrics...\n",
      "✓ Think token sensitivity analyzed\n",
      "✓ Critical layers identified: [0, 16, 40, 44]\n",
      "✓ Critical layers (think tokens): [8, 20, 24, 44]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 5: Evaluating results with improved metrics...\")\n",
    "\n",
    "# Analyze layer sensitivity using token count\n",
    "sensitivity_tokens = evaluator.analyze_layer_sensitivity(intervention_results, metric='token_count')\n",
    "\n",
    "# NEW: Analyze layer sensitivity using quality metrics\n",
    "# Extract think token counts for sensitivity analysis\n",
    "think_token_results = []\n",
    "for result in intervention_results:\n",
    "    if 'tokens' in result:\n",
    "        think_token_results.append({\n",
    "            'layer': result['layer'],\n",
    "            'think_tokens': result['tokens']['think_tokens']\n",
    "        })\n",
    "\n",
    "if think_token_results:\n",
    "    sensitivity_think = evaluator.analyze_layer_sensitivity(think_token_results, metric='think_tokens')\n",
    "    print(f\"✓ Think token sensitivity analyzed\")\n",
    "\n",
    "critical_layers = evaluator.identify_critical_layers(sensitivity_tokens)\n",
    "print(f\"✓ Critical layers identified: {critical_layers}\")\n",
    "\n",
    "if think_token_results:\n",
    "    critical_layers_think = evaluator.identify_critical_layers(sensitivity_think)\n",
    "    print(f\"✓ Critical layers (think tokens): {critical_layers_think}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Analyzing reasoning quality changes...\n",
      "✓ Quality analysis completed for 16 layers\n",
      "\n",
      "Top 5 layers by quality metric variance:\n",
      "  Layer 20: total_var=90429.41\n",
      "  Layer 32: total_var=85702.86\n",
      "  Layer 24: total_var=77782.69\n",
      "  Layer 44: total_var=75115.14\n",
      "  Layer 0: total_var=71630.02\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 6: Analyzing reasoning quality changes...\")\n",
    "\n",
    "# Analyze quality metric variations by layer\n",
    "quality_by_layer = {}\n",
    "for layer in layers_to_test:\n",
    "    layer_results = [r for r in intervention_results if r['layer'] == layer and 'quality' in r]\n",
    "    if layer_results:\n",
    "        quality_by_layer[layer] = {\n",
    "            'reasoning_steps': [r['quality']['reasoning_steps'] for r in layer_results],\n",
    "            'backtracking': [r['quality']['backtracking_count'] for r in layer_results],\n",
    "            'hesitation': [r['quality']['hesitation_count'] for r in layer_results],\n",
    "            'verbosity': [r['quality']['verbosity'] for r in layer_results]\n",
    "        }\n",
    "\n",
    "print(f\"✓ Quality analysis completed for {len(quality_by_layer)} layers\")\n",
    "\n",
    "# Find layers with most quality variation\n",
    "import numpy as np\n",
    "quality_variance = {}\n",
    "for layer, metrics in quality_by_layer.items():\n",
    "    quality_variance[layer] = {\n",
    "        'reasoning_steps_var': np.var(metrics['reasoning_steps']),\n",
    "        'backtracking_var': np.var(metrics['backtracking']),\n",
    "        'hesitation_var': np.var(metrics['hesitation']),\n",
    "        'verbosity_var': np.var(metrics['verbosity']),\n",
    "        'total_var': sum([\n",
    "            np.var(metrics['reasoning_steps']),\n",
    "            np.var(metrics['backtracking']),\n",
    "            np.var(metrics['hesitation']),\n",
    "            np.var(metrics['verbosity'])\n",
    "        ])\n",
    "    }\n",
    "\n",
    "# Sort layers by total quality variance\n",
    "sorted_quality_layers = sorted(quality_variance.items(), key=lambda x: x[1]['total_var'], reverse=True)\n",
    "print(\"\\nTop 5 layers by quality metric variance:\")\n",
    "for layer, var in sorted_quality_layers[:5]:\n",
    "    print(f\"  Layer {layer}: total_var={var['total_var']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Enhanced Report and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: Generating enhanced report...\n",
      "Report saved to /scratch/gilbreth/sramishe/results_QwQ_R1/results_improved/evaluation_report.txt\n",
      "✓ Results saved to /scratch/gilbreth/sramishe/results_QwQ_R1/results_improved\n",
      "\n",
      "============================================================\n",
      "IMPROVED PIPELINE COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "Total interventions: 144\n",
      "Critical layers: [0, 16, 40, 44]\n",
      "Quality-sensitive layers: [20, 32, 24, 44, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 7: Generating enhanced report...\")\n",
    "\n",
    "# Generate evaluation report\n",
    "report = evaluator.generate_report(\n",
    "    intervention_results,\n",
    "    output_file=str(output_path / \"evaluation_report.txt\")\n",
    ")\n",
    "\n",
    "# Save comprehensive results\n",
    "results_data = {\n",
    "    'config': CONFIG,\n",
    "    'model_info': model_info,\n",
    "    'directions_stats': calculator.compute_direction_stats(),\n",
    "    'baselines': baselines,\n",
    "    'critical_layers': critical_layers,\n",
    "    'critical_layers_think': critical_layers_think if think_token_results else [],\n",
    "    'layer_sensitivity': sensitivity_tokens,\n",
    "    'layer_sensitivity_think': sensitivity_think if think_token_results else {},\n",
    "    'quality_variance': quality_variance,\n",
    "    'top_quality_layers': [(layer, var['total_var']) for layer, var in sorted_quality_layers[:10]],\n",
    "    'intervention_results': intervention_results\n",
    "}\n",
    "\n",
    "# Save with proper serialization\n",
    "with open(output_path / \"results.json\", 'w') as f:\n",
    "    json.dump(results_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"✓ Results saved to {output_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPROVED PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal interventions: {len(intervention_results)}\")\n",
    "print(f\"Critical layers: {critical_layers}\")\n",
    "print(f\"Quality-sensitive layers: {[layer for layer, _ in sorted_quality_layers[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Compare Baseline vs Best/Worst Interventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Suppressed Reasoning:\n",
      "  Layer 0, strength -1.00: +0 steps\n",
      "  Layer 0, strength 0.00: +0 steps\n",
      "  Layer 0, strength 0.50: +0 steps\n",
      "\n",
      "Most Enhanced Reasoning:\n",
      "  Layer 52, strength 0.50: +22 steps\n",
      "  Layer 28, strength -0.50: +24 steps\n",
      "  Layer 52, strength -1.50: +29 steps\n"
     ]
    }
   ],
   "source": [
    "# Find interventions with most extreme effects\n",
    "if intervention_results and 'quality' in intervention_results[0]:\n",
    "    baseline_quality = baselines[0]['quality']\n",
    "    \n",
    "    # Calculate quality deltas\n",
    "    for result in intervention_results:\n",
    "        if 'quality' in result:\n",
    "            result['reasoning_delta'] = result['quality']['reasoning_steps'] - baseline_quality['reasoning_steps']\n",
    "            result['backtracking_delta'] = result['quality']['backtracking_count'] - baseline_quality['backtracking_count']\n",
    "    \n",
    "    # Sort by reasoning steps delta\n",
    "    sorted_by_reasoning = sorted(\n",
    "        [r for r in intervention_results if 'reasoning_delta' in r],\n",
    "        key=lambda x: x['reasoning_delta']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMost Suppressed Reasoning:\")\n",
    "    for r in sorted_by_reasoning[:3]:\n",
    "        print(f\"  Layer {r['layer']}, strength {r['strength']:.2f}: {r['reasoning_delta']:+d} steps\")\n",
    "    \n",
    "    print(\"\\nMost Enhanced Reasoning:\")\n",
    "    for r in sorted_by_reasoning[-3:]:\n",
    "        print(f\"  Layer {r['layer']}, strength {r['strength']:.2f}: {r['reasoning_delta']:+d} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Inverse Direction Strategy:\n",
      "  Model: Qwen/QwQ-32B\n",
      "  Strength range: (-2.0, 0.0) (negative = toward distilled)\n",
      "  Test layers: [20, 30, 40]\n",
      "  Test problems: 10\n",
      "\n",
      "Expectation: Negative strengths should improve performance\n",
      "  - Baseline (strength=0): ~31.80% accuracy\n",
      "  - With intervention (strength=-2.0): hopefully > 31.80%\n"
     ]
    }
   ],
   "source": [
    "# Configuration for inverse direction test\n",
    "CONFIG = {\n",
    "    'rl_model_name': \"Qwen/QwQ-32B\",\n",
    "    'strength_range': (-2.0, 0.0),  # NEGATIVE to move toward distilled\n",
    "    'num_strengths': 5,  # [-2.0, -1.5, -1.0, -0.5, 0.0]\n",
    "    'test_layers': [20, 30, 40],  # Test a few key layers\n",
    "    'num_test_problems': 10,  # Test on 10 MATH-500 problems\n",
    "    'max_new_tokens': 1024,\n",
    "    'output_dir': '/scratch/gilbreth/sramishe/results_QwQ_R1/inverse_test'\n",
    "}\n",
    "\n",
    "print(\"Testing Inverse Direction Strategy:\")\n",
    "print(f\"  Model: {CONFIG['rl_model_name']}\")\n",
    "print(f\"  Strength range: {CONFIG['strength_range']} (negative = toward distilled)\")\n",
    "print(f\"  Test layers: {CONFIG['test_layers']}\")\n",
    "print(f\"  Test problems: {CONFIG['num_test_problems']}\")\n",
    "print(f\"\\nExpectation: Negative strengths should improve performance\")\n",
    "print(f\"  - Baseline (strength=0): ~31.80% accuracy\")\n",
    "print(f\"  - With intervention (strength=-2.0): hopefully > 31.80%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Loading dataset: HuggingFaceH4/MATH-500\n",
      "✓ Model loaded\n",
      "✓ 10 test problems prepared\n",
      "\n",
      "Testing layer 20...\n",
      "  Strength: -2.00 Applied interventions to layers: [20]\n",
      "→ think_tokens: 0, steps: 0\n",
      "  Strength: -1.50 Applied interventions to layers: [20]\n",
      "→ think_tokens: 942, steps: 27\n",
      "  Strength: -1.00 Applied interventions to layers: [20]\n",
      "→ think_tokens: 1019, steps: 22\n",
      "  Strength: -0.50 Applied interventions to layers: [20]\n",
      "→ think_tokens: 910, steps: 19\n",
      "  Strength: 0.00 Applied interventions to layers: [20]\n",
      "→ think_tokens: 0, steps: 0\n",
      "Skipping layer 30 (no direction available)\n",
      "\n",
      "Testing layer 40...\n",
      "  Strength: -2.00 Applied interventions to layers: [40]\n",
      "→ think_tokens: 0, steps: 0\n",
      "  Strength: -1.50 Applied interventions to layers: [40]\n",
      "→ think_tokens: 0, steps: 0\n",
      "  Strength: -1.00 Applied interventions to layers: [40]\n",
      "→ think_tokens: 0, steps: 0\n",
      "  Strength: -0.50 Applied interventions to layers: [40]\n",
      "→ think_tokens: 0, steps: 0\n",
      "  Strength: 0.00 Applied interventions to layers: [40]\n",
      "→ think_tokens: 0, steps: 0\n",
      "\n",
      "✓ Completed 10 experiments\n"
     ]
    }
   ],
   "source": [
    "# Run intervention sweep with NEGATIVE strengths\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "processor = DataProcessor(\n",
    "    dataset_name=\"HuggingFaceH4/MATH-500\",\n",
    "    include_toy_tasks=False\n",
    ")\n",
    "dataset = processor.load_dataset(max_samples=CONFIG['num_test_problems'])\n",
    "test_prompts = processor.prepare_batch(dataset[:CONFIG['num_test_problems']], rl_tokenizer)\n",
    "\n",
    "print(f\"✓ Model loaded\")\n",
    "print(f\"✓ {len(test_prompts)} test problems prepared\")\n",
    "\n",
    "evaluator = ReasoningEvaluator()\n",
    "results = []\n",
    "\n",
    "# Test each layer\n",
    "for layer in CONFIG['test_layers']:\n",
    "    if layer not in directions:\n",
    "        print(f\"Skipping layer {layer} (no direction available)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nTesting layer {layer}...\")\n",
    "    patcher = ActivationPatcher(rl_model, directions)\n",
    "    \n",
    "    # Test each strength\n",
    "    strengths = np.linspace(CONFIG['strength_range'][0], CONFIG['strength_range'][1], CONFIG['num_strengths'])\n",
    "    \n",
    "    for strength in strengths:\n",
    "        print(f\"  Strength: {strength:.2f}\", end=\" \")\n",
    "        \n",
    "        # Generate with intervention\n",
    "        output = patcher.generate_with_intervention(\n",
    "            tokenizer=rl_tokenizer,\n",
    "            prompt=test_prompts[0],  # Use first test problem\n",
    "            layers=[layer],\n",
    "            strength=float(strength),\n",
    "            max_new_tokens=CONFIG['max_new_tokens']\n",
    "        )\n",
    "        \n",
    "        # Evaluate quality\n",
    "        tokens = evaluator.count_tokens(output, rl_tokenizer, split_by_tags=True)\n",
    "        quality = evaluator.analyze_reasoning_quality(output)\n",
    "        \n",
    "        result = {\n",
    "            'layer': layer,\n",
    "            'strength': float(strength),\n",
    "            'output': output,\n",
    "            'tokens': tokens,\n",
    "            'quality': quality\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"→ think_tokens: {tokens['think_tokens']}, steps: {quality['reasoning_steps']}\")\n",
    "\n",
    "print(f\"\\n✓ Completed {len(results)} experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results Summary:\n",
      " layer  strength  total_tokens  think_tokens  reasoning_steps  backtracking  hesitation\n",
      "    20      -2.0          1078             0                0             0           0\n",
      "    20      -1.5          1078           942               27             3           1\n",
      "    20      -1.0          1078          1019               22             2           2\n",
      "    20      -0.5          1078           910               19             2           2\n",
      "    20       0.0          1078             0                0             0           0\n",
      "    40      -2.0          1078             0                0             0           0\n",
      "    40      -1.5          1078             0                0             0           0\n",
      "    40      -1.0          1078             0                0             0           0\n",
      "    40      -0.5          1078             0                0             0           0\n",
      "    40       0.0          1078             0                0             0           0\n",
      "\n",
      "============================================================\n",
      "ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Layer 20:\n",
      "  Baseline (strength=0.0):\n",
      "    Think tokens: 0.0\n",
      "    Reasoning steps: 0.0\n",
      "  Best (strength=-1.50):\n",
      "    Think tokens: 942.0 (+942.0)\n",
      "    Reasoning steps: 27.0 (+27.0)\n",
      "\n",
      "Layer 30: no results found, skipping\n",
      "\n",
      "Layer 40:\n",
      "  Baseline (strength=0.0):\n",
      "    Think tokens: 0.0\n",
      "    Reasoning steps: 0.0\n",
      "  Best (strength=-2.00):\n",
      "    Think tokens: 0.0 (+0.0)\n",
      "    Reasoning steps: 0.0 (+0.0)\n"
     ]
    }
   ],
   "source": [
    "# Analyze results\n",
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "df_results = pd.DataFrame([{\n",
    "    'layer': r['layer'],\n",
    "    'strength': r['strength'],\n",
    "    'total_tokens': r['tokens']['total_tokens'],\n",
    "    'think_tokens': r['tokens']['think_tokens'],\n",
    "    'reasoning_steps': r['quality']['reasoning_steps'],\n",
    "    'backtracking': r['quality']['backtracking_count'],\n",
    "    'hesitation': r['quality']['hesitation_count'],\n",
    "} for r in results])\n",
    "\n",
    "print(\"\\nResults Summary:\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Find best configurations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# for layer in CONFIG['test_layers']:\n",
    "#     layer_results = df_results[df_results['layer'] == layer]\n",
    "#     baseline = layer_results[layer_results['strength'] == 0.0].iloc[0]\n",
    "#     best = layer_results.loc[layer_results['reasoning_steps'].idxmax()]\n",
    "    \n",
    "#     print(f\"\\nLayer {layer}:\")\n",
    "#     print(f\"  Baseline (strength=0.0):\")\n",
    "#     print(f\"    Think tokens: {baseline['think_tokens']}\")\n",
    "#     print(f\"    Reasoning steps: {baseline['reasoning_steps']}\")\n",
    "#     print(f\"  Best (strength={best['strength']:.2f}):\")\n",
    "#     # print(f\"    Think tokens: {best['think_tokens']} ({best['think_tokens'] - baseline['think_tokens']:+.1f})\")\n",
    "#     # print(f\"    Reasoning steps: {best['reasoning_steps']} ({best['reasoning_steps'] - baseline['reasoning_steps']:+.1f})\")\n",
    "\n",
    "#     print(\n",
    "#     f\"    Think tokens: {best['think_tokens']:.1f} \"\n",
    "#     f\"({best['think_tokens'] - baseline['think_tokens']:+.1f})\"\n",
    "#     )\n",
    "#     print(\n",
    "#     f\"    Reasoning steps: {best['reasoning_steps']:.1f} \"\n",
    "#     f\"({best['reasoning_steps'] - baseline['reasoning_steps']:+.1f})\"\n",
    "#     )\n",
    "\n",
    "for layer in CONFIG['test_layers']:\n",
    "    layer_results = df_results[df_results['layer'] == layer]\n",
    "\n",
    "    if layer_results.empty:\n",
    "        print(f\"\\nLayer {layer}: no results found, skipping\")\n",
    "        continue\n",
    "\n",
    "    baseline_rows = layer_results[layer_results['strength'] == 0.0]\n",
    "\n",
    "    if baseline_rows.empty:\n",
    "        print(f\"\\nLayer {layer}: no baseline (strength=0.0), skipping\")\n",
    "        continue\n",
    "\n",
    "    baseline = baseline_rows.iloc[0]\n",
    "    best = layer_results.loc[layer_results['reasoning_steps'].idxmax()]\n",
    "\n",
    "    print(f\"\\nLayer {layer}:\")\n",
    "    print(f\"  Baseline (strength=0.0):\")\n",
    "    print(f\"    Think tokens: {baseline['think_tokens']}\")\n",
    "    print(f\"    Reasoning steps: {baseline['reasoning_steps']}\")\n",
    "    print(f\"  Best (strength={best['strength']:.2f}):\")\n",
    "    print(\n",
    "        f\"    Think tokens: {best['think_tokens']:.1f} \"\n",
    "        f\"({best['think_tokens'] - baseline['think_tokens']:+.1f})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"    Reasoning steps: {best['reasoning_steps']:.1f} \"\n",
    "        f\"({best['reasoning_steps'] - baseline['reasoning_steps']:+.1f})\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to /scratch/gilbreth/sramishe/results_QwQ_R1/inverse_test/inverse_direction_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "output_path = Path(CONFIG['output_dir'])\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path / 'inverse_direction_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to {output_path / 'inverse_direction_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../pipeline')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Import intervention and evaluator\n",
    "from intervention import ActivationPatcher\n",
    "from evaluator import ReasoningEvaluator\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 6 intervention configurations\n",
      "Output directory: /scratch/gilbreth/sramishe/results_QwQ_R1/interventions_math500\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "RL_MODEL = \"Qwen/QwQ-32B\"\n",
    "DATASET = \"HuggingFaceH4/MATH-500\"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\"/scratch/gilbreth/sramishe/results_QwQ_R1/interventions_math500\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Intervention configurations to test\n",
    "INTERVENTIONS = [\n",
    "    {\"name\": \"baseline\", \"layer\": None, \"strength\": 0.0},\n",
    "    {\"name\": \"L16_s-2.0_thorough\", \"layer\": 16, \"strength\": -2.0},\n",
    "    {\"name\": \"L0_s-1.5_distilled\", \"layer\": 0, \"strength\": -1.5},\n",
    "    {\"name\": \"L20_s-1.0_concise\", \"layer\": 20, \"strength\": -1.0},\n",
    "    {\"name\": \"L0_s-2.0_distilled_strong\", \"layer\": 0, \"strength\": -2.0},\n",
    "    {\"name\": \"L16_s-0.5_moderate\", \"layer\": 16, \"strength\": -0.5},\n",
    "]\n",
    "\n",
    "# Generation parameters\n",
    "MAX_NEW_TOKENS = 2048  # Increased to avoid truncation\n",
    "TEMPERATURE = 0.0  # Greedy decoding for reproducibility\n",
    "\n",
    "print(f\"Testing {len(INTERVENTIONS)} intervention configurations\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MATH-500 dataset...\n",
      "Dataset loaded: 500 problems\n",
      "Sample problem:\n",
      "  Question: Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the...\n",
      "  Answer: \\left( 3, \\frac{\\pi}{2} \\right)\n"
     ]
    }
   ],
   "source": [
    "# Load MATH-500\n",
    "print(\"Loading MATH-500 dataset...\")\n",
    "dataset = load_dataset(DATASET, split=\"test\")\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} problems\")\n",
    "print(f\"Sample problem:\")\n",
    "print(f\"  Question: {dataset[0]['problem'][:100]}...\")\n",
    "print(f\"  Answer: {dataset[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded: Qwen/QwQ-32B\n",
      "  Layers: 64\n",
      "  Hidden size: 5120\n"
     ]
    }
   ],
   "source": [
    "print(f\"✓ Model loaded: {RL_MODEL}\")\n",
    "print(f\"  Layers: {rl_model.config.num_hidden_layers}\")\n",
    "print(f\"  Hidden size: {rl_model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reasoning directions...\n",
      "✓ Loaded directions for 16 layers\n",
      "  Available layers: [0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60]\n"
     ]
    }
   ],
   "source": [
    "# Load reasoning directions\n",
    "print(\"Loading reasoning directions...\")\n",
    "directions_path = Path(\"/scratch/gilbreth/sramishe/results_QwQ_R1/results_improved/reasoning_directions.pt\")\n",
    "\n",
    "if directions_path.exists():\n",
    "    directions = torch.load(directions_path)\n",
    "    print(f\"✓ Loaded directions for {len(directions)} layers\")\n",
    "    print(f\"  Available layers: {sorted(directions.keys())}\")\n",
    "else:\n",
    "    print(\"⚠️  Directions file not found!\")\n",
    "    print(\"   Need to run direction extraction first\")\n",
    "    raise FileNotFoundError(f\"Directions not found at {directions_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Grading functions defined\n",
      "\n",
      "Testing grading function:\n",
      "  ✓ grade_answer('42', '42') = True (expected True)\n",
      "  ✓ grade_answer('1/2', '0.5') = True (expected True)\n",
      "  ✗ grade_answer('\\frac{1}{2}', '0.5') = False (expected True)\n",
      "  ✓ grade_answer('42', '43') = False (expected False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sramishe/.venv llm-env/lib64/python3.9/site-packages/sympy/parsing/latex/_parse_latex_antlr.py:23: UserWarning: antlr4.error.ErrorListener module is not installed\n",
      "  ErrorListener = import_module('antlr4.error.ErrorListener',\n"
     ]
    }
   ],
   "source": [
    "from sympy import simplify, sympify, N\n",
    "from sympy.parsing.latex import parse_latex\n",
    "\n",
    "def extract_answer(text):\n",
    "    \"\"\"\n",
    "    Extract answer from model output.\n",
    "    Looks for \\boxed{} format first, then falls back to last number.\n",
    "    \"\"\"\n",
    "    # Look for \\boxed{...}\n",
    "    boxed_pattern = r'\\\\boxed\\{([^}]+)\\}'\n",
    "    boxed_matches = re.findall(boxed_pattern, text)\n",
    "    if boxed_matches:\n",
    "        return boxed_matches[-1].strip()\n",
    "    \n",
    "    # Fallback: find last number-like pattern\n",
    "    number_pattern = r'[-+]?\\d*\\.?\\d+'\n",
    "    number_matches = re.findall(number_pattern, text)\n",
    "    if number_matches:\n",
    "        return number_matches[-1].strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def normalize_answer(answer):\n",
    "    \"\"\"\n",
    "    Normalize answer for comparison.\n",
    "    Handles LaTeX, fractions, etc.\n",
    "    \"\"\"\n",
    "    if answer is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Try parsing as LaTeX first\n",
    "        if '\\\\' in answer:\n",
    "            expr = parse_latex(answer)\n",
    "        else:\n",
    "            expr = sympify(answer)\n",
    "        \n",
    "        # Simplify\n",
    "        simplified = simplify(expr)\n",
    "        return simplified\n",
    "    except:\n",
    "        # If parsing fails, return string\n",
    "        return answer.strip().lower()\n",
    "\n",
    "def grade_answer(predicted, ground_truth):\n",
    "    \"\"\"\n",
    "    Grade predicted answer against ground truth.\n",
    "    Returns True if correct, False otherwise.\n",
    "    \"\"\"\n",
    "    pred_norm = normalize_answer(predicted)\n",
    "    truth_norm = normalize_answer(ground_truth)\n",
    "    \n",
    "    if pred_norm is None or truth_norm is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Try symbolic comparison\n",
    "        return simplify(pred_norm - truth_norm) == 0\n",
    "    except:\n",
    "        # Fallback to string comparison\n",
    "        return str(pred_norm) == str(truth_norm)\n",
    "\n",
    "print(\"✓ Grading functions defined\")\n",
    "\n",
    "# Test grading\n",
    "test_cases = [\n",
    "    (\"42\", \"42\", True),\n",
    "    (\"1/2\", \"0.5\", True),\n",
    "    (\"\\\\frac{1}{2}\", \"0.5\", True),\n",
    "    (\"42\", \"43\", False),\n",
    "]\n",
    "\n",
    "print(\"\\nTesting grading function:\")\n",
    "for pred, truth, expected in test_cases:\n",
    "    result = grade_answer(pred, truth)\n",
    "    status = \"✓\" if result == expected else \"✗\"\n",
    "    print(f\"  {status} grade_answer('{pred}', '{truth}') = {result} (expected {expected})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "def evaluate_intervention(intervention_config, dataset, model, tokenizer, directions, evaluator):\n",
    "    \"\"\"\n",
    "    Evaluate a single intervention configuration on MATH-500.\n",
    "    \n",
    "    Returns:\n",
    "        dict with accuracy, results, and metadata\n",
    "    \"\"\"\n",
    "    name = intervention_config['name']\n",
    "    layer = intervention_config['layer']\n",
    "    strength = intervention_config['strength']\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating: {name}\")\n",
    "    if layer is not None:\n",
    "        print(f\"  Layer: {layer}, Strength: {strength}\")\n",
    "    else:\n",
    "        print(f\"  Baseline (no intervention)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Initialize intervention\n",
    "    if layer is not None:\n",
    "        patcher = ActivationPatcher(\n",
    "            model=model,\n",
    "            directions={layer: directions[layer]}\n",
    "        )\n",
    "    else:\n",
    "        patcher = None\n",
    "    \n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(dataset, desc=f\"Evaluating {name}\")):\n",
    "        problem = sample['problem']\n",
    "        ground_truth = sample['answer']\n",
    "        \n",
    "        # Format prompt\n",
    "        prompt = f\"\"\"user: {problem}\n",
    "assistant:\"\"\"\n",
    "        \n",
    "        # Generate with intervention\n",
    "        if patcher is not None:\n",
    "            output = patcher.generate_with_intervention(\n",
    "                tokenizer=tokenizer,\n",
    "                prompt=prompt,\n",
    "                layers=[layer],  # layers is a list\n",
    "                strength=strength,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            # Remove prompt from output\n",
    "            output = output[len(prompt):].strip()\n",
    "        else:\n",
    "            # Baseline generation\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            output = output[len(prompt):].strip()\n",
    "        \n",
    "        # Extract answer\n",
    "        predicted_answer = extract_answer(output)\n",
    "        \n",
    "        # Grade\n",
    "        is_correct = grade_answer(predicted_answer, ground_truth)\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        # Get quality metrics\n",
    "        tokens = evaluator.count_tokens(output, tokenizer, split_by_tags=True)\n",
    "        quality = evaluator.analyze_reasoning_quality(output)\n",
    "        \n",
    "        # Store result\n",
    "        result = {\n",
    "            'problem_id': i,\n",
    "            'problem': problem,\n",
    "            'ground_truth': ground_truth,\n",
    "            'output': output,\n",
    "            'predicted_answer': predicted_answer,\n",
    "            'is_correct': is_correct,\n",
    "            'tokens': tokens,\n",
    "            'quality': quality\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # Print progress every 50 problems\n",
    "        if (i + 1) % 50 == 0:\n",
    "            current_acc = 100.0 * correct / total\n",
    "            print(f\"  Progress: {i+1}/{len(dataset)}, Accuracy: {current_acc:.2f}% ({correct}/{total})\")\n",
    "    \n",
    "    # Calculate final accuracy\n",
    "    accuracy = 100.0 * correct / total\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RESULTS: {name}\")\n",
    "    print(f\"  Correct: {correct}/{total}\")\n",
    "    print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"  Baseline: 31.80%\")\n",
    "    print(f\"  Improvement: {accuracy - 31.80:+.2f}pp\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return {\n",
    "        'config': intervention_config,\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'results': results\n",
    "    }\n",
    "\n",
    "print(\"✓ Evaluation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluator initialized (with fixed extract_think_tags)\n"
     ]
    }
   ],
   "source": [
    "evaluator = ReasoningEvaluator()\n",
    "print(\"✓ Evaluator initialized (with fixed extract_think_tags)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚠️  TESTING ON SUBSET: First 50 problems\n",
      "   Change TEST_SUBSET=False to run on full dataset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OPTION 1: Quick test on subset (recommended first)\n",
    "TEST_SUBSET = True\n",
    "SUBSET_SIZE = 50  # Test on first 50 problems\n",
    "\n",
    "if TEST_SUBSET:\n",
    "    print(f\"\\n⚠️  TESTING ON SUBSET: First {SUBSET_SIZE} problems\")\n",
    "    print(f\"   Change TEST_SUBSET=False to run on full dataset\\n\")\n",
    "    test_dataset = dataset.select(range(SUBSET_SIZE))\n",
    "else:\n",
    "    print(\"\\n✓ Running on FULL DATASET (500 problems)\\n\")\n",
    "    test_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluating: baseline\n",
      "  Baseline (no intervention)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating baseline:   0%|          | 0/50 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating baseline: 100%|██████████| 50/50 [1:25:34<00:00, 102.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Progress: 50/50, Accuracy: 40.00% (20/50)\n",
      "\n",
      "================================================================================\n",
      "RESULTS: baseline\n",
      "  Correct: 20/50\n",
      "  Accuracy: 40.00%\n",
      "  Baseline: 31.80%\n",
      "  Improvement: +8.20pp\n",
      "================================================================================\n",
      "\n",
      "✓ Saved results to /scratch/gilbreth/sramishe/results_QwQ_R1/interventions_math500/baseline_results.json\n",
      "\n",
      "================================================================================\n",
      "Evaluating: L16_s-2.0_thorough\n",
      "  Layer: 16, Strength: -2.0\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:   2%|▏         | 1/50 [01:43<1:24:11, 103.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:   4%|▍         | 2/50 [03:31<1:24:53, 106.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:   6%|▌         | 3/50 [05:19<1:23:50, 107.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:   8%|▊         | 4/50 [07:07<1:22:20, 107.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  10%|█         | 5/50 [08:56<1:20:56, 107.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  12%|█▏        | 6/50 [10:44<1:19:10, 107.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  14%|█▍        | 7/50 [12:32<1:17:23, 107.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  16%|█▌        | 8/50 [14:20<1:15:37, 108.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  18%|█▊        | 9/50 [15:48<1:09:26, 101.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  20%|██        | 10/50 [17:36<1:09:07, 103.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  22%|██▏       | 11/50 [19:24<1:08:16, 105.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  24%|██▍       | 12/50 [21:12<1:07:06, 105.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  26%|██▌       | 13/50 [23:00<1:05:45, 106.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  28%|██▊       | 14/50 [24:28<1:00:37, 101.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  30%|███       | 15/50 [26:17<1:00:12, 103.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  32%|███▏      | 16/50 [28:05<59:22, 104.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  34%|███▍      | 17/50 [29:53<58:09, 105.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  36%|███▌      | 18/50 [31:41<56:50, 106.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  38%|███▊      | 19/50 [33:30<55:20, 107.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  40%|████      | 20/50 [35:18<53:42, 107.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  42%|████▏     | 21/50 [37:06<51:59, 107.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  44%|████▍     | 22/50 [38:54<50:16, 107.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  46%|████▌     | 23/50 [40:42<48:32, 107.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  48%|████▊     | 24/50 [42:30<46:46, 107.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  50%|█████     | 25/50 [44:18<45:00, 108.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  52%|█████▏    | 26/50 [46:07<43:13, 108.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  54%|█████▍    | 27/50 [47:55<41:26, 108.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  56%|█████▌    | 28/50 [49:09<35:55, 97.96s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  58%|█████▊    | 29/50 [50:57<35:20, 100.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  60%|██████    | 30/50 [52:45<34:22, 103.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  62%|██████▏   | 31/50 [54:33<33:07, 104.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  64%|██████▍   | 32/50 [56:21<31:41, 105.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  66%|██████▌   | 33/50 [58:09<30:08, 106.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  68%|██████▊   | 34/50 [59:58<28:30, 106.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  70%|███████   | 35/50 [1:01:46<26:48, 107.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  72%|███████▏  | 36/50 [1:03:34<25:04, 107.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  74%|███████▍  | 37/50 [1:05:22<23:19, 107.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  76%|███████▌  | 38/50 [1:07:10<21:33, 107.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  78%|███████▊  | 39/50 [1:08:46<19:07, 104.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  80%|████████  | 40/50 [1:10:34<17:34, 105.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  82%|████████▏ | 41/50 [1:12:03<15:04, 100.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating L16_s-2.0_thorough:  84%|████████▍ | 42/50 [1:13:52<13:42, 102.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied interventions to layers: [16]\n"
     ]
    }
   ],
   "source": [
    "# Run all evaluations\n",
    "all_results = {}\n",
    "\n",
    "for intervention in INTERVENTIONS:\n",
    "    name = intervention['name']\n",
    "    \n",
    "    # Check if already computed\n",
    "    result_file = OUTPUT_DIR / f\"{name}_results.json\"\n",
    "    if result_file.exists():\n",
    "        print(f\"\\n⚠️  Skipping {name} - results already exist at {result_file}\")\n",
    "        print(f\"   Delete the file to re-run\\n\")\n",
    "        with open(result_file, 'r') as f:\n",
    "            all_results[name] = json.load(f)\n",
    "        continue\n",
    "    \n",
    "    # Run evaluation\n",
    "    result = evaluate_intervention(\n",
    "        intervention_config=intervention,\n",
    "        dataset=test_dataset,\n",
    "        model=rl_model,\n",
    "        tokenizer=rl_tokenizer,\n",
    "        directions=directions,\n",
    "        evaluator=evaluator\n",
    "    )\n",
    "    \n",
    "    # Save individual result\n",
    "    with open(result_file, 'w') as f:\n",
    "        json.dump(result, f, indent=2, default=str)\n",
    "    print(f\"\\n✓ Saved results to {result_file}\")\n",
    "    \n",
    "    all_results[name] = result\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL EVALUATIONS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
