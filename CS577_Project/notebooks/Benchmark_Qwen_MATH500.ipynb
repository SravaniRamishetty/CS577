{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set custom cache directory for Hugging Face transformers\n",
    "# This avoids using the default home directory and stores models in scratch space\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/scratch/gilbreth/sramishe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sramishe/.venv llm-env/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sramishe/.venv llm-env/lib64/python3.9/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset  # For loading the MATH-500 benchmark dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # For loading LLM model and tokenizer\n",
    "from tqdm import tqdm  # Progress bar for batch processing\n",
    "from sympy import simplify  # For symbolic math comparison\n",
    "from sympy.parsing import sympy_parser as spp  # For parsing mathematical expressions\n",
    "from sympy.core.sympify import SympifyError  # For handling parsing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA A100 80GB PCIe\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "# GPU Configuration Check\n",
    "# Verify CUDA availability and print GPU information\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Number of GPUs\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "# Name of each GPU (should show NVIDIA A100 80GB PCIe)\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Current GPU device index\n",
    "print(\"Current device:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ReasoningConfig:\n",
    "    \"\"\"Configuration for reasoning model evaluation\"\"\"\n",
    "    model_name: str = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"  # The reasoning model to benchmark\n",
    "    dataset_name: str = \"HuggingFaceH4/MATH-500\"  # Math benchmark dataset with 500 problems\n",
    "    dataset_split: str = \"test\"  # Use test split for evaluation\n",
    "    batch_size: int = 2  # Number of samples per batch (reduced due to large model size)\n",
    "    max_new_tokens: int = 512  # Maximum tokens to generate for reasoning and answer\n",
    "    limit_eval_samples: Optional[int] = 500  # Limit samples for faster testing (None = use all)\n",
    "    output_file: Optional[str] = \"math500_results_500.jsonl\"  # File to save detailed results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS FROM RASCHKA'S CODE\n",
    "# ============================================================================\n",
    "\n",
    "def get_last_boxed(text):\n",
    "    \"\"\"\n",
    "    Extract the content inside the last \\\\boxed{...} in the text.\n",
    "    Handles nested braces correctly.\n",
    "    \n",
    "    Args:\n",
    "        text: Generated text that may contain \\\\boxed{answer}\n",
    "    \n",
    "    Returns:\n",
    "        Content inside the last \\\\boxed{} or None if not found\n",
    "    \"\"\"\n",
    "    # Find the last occurrence of \"\\boxed\"\n",
    "    boxed_start_idx = text.rfind(r\"\\boxed\")\n",
    "    if boxed_start_idx == -1:\n",
    "        return None\n",
    "\n",
    "    # Get position after \"\\boxed\"\n",
    "    current_idx = boxed_start_idx + len(r\"\\boxed\")\n",
    "\n",
    "    # Skip any whitespace after \"\\boxed\"\n",
    "    while current_idx < len(text) and text[current_idx].isspace():\n",
    "        current_idx += 1\n",
    "\n",
    "    # Expect an opening brace \"{\"\n",
    "    if current_idx >= len(text) or text[current_idx] != \"{\":\n",
    "        return None\n",
    "\n",
    "    # Parse the braces with nesting support\n",
    "    current_idx += 1\n",
    "    brace_depth = 1\n",
    "    content_start_idx = current_idx\n",
    "\n",
    "    while current_idx < len(text) and brace_depth > 0:\n",
    "        char = text[current_idx]\n",
    "        if char == \"{\":\n",
    "            brace_depth += 1\n",
    "        elif char == \"}\":\n",
    "            brace_depth -= 1\n",
    "        current_idx += 1\n",
    "\n",
    "    # Check for unbalanced braces\n",
    "    if brace_depth != 0:\n",
    "        return None\n",
    "\n",
    "    # Extract content inside the outermost braces\n",
    "    return text[content_start_idx:current_idx-1]\n",
    "\n",
    "\n",
    "# Regex pattern for extracting numbers (including fractions, decimals, scientific notation)\n",
    "RE_NUMBER = re.compile(r\"-?(?:\\d+/\\d+|\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?)\")\n",
    "\n",
    "def extract_final_candidate(text, fallback=\"number_then_full\"):\n",
    "    \"\"\"\n",
    "    Extract the final answer from generated text.\n",
    "    Prefers \\\\boxed{} format, falls back to extracting last number.\n",
    "    \n",
    "    Args:\n",
    "        text: Generated text from the model\n",
    "        fallback: Strategy when \\\\boxed{} not found (\"number_then_full\" or \"number_only\")\n",
    "    \n",
    "    Returns:\n",
    "        Extracted answer string\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "\n",
    "    if text:\n",
    "        # Prefer the last boxed expression if present\n",
    "        boxed = get_last_boxed(text.strip())\n",
    "        if boxed:\n",
    "            result = boxed.strip().strip(\"$ \")\n",
    "        # If no boxed expression, try fallback\n",
    "        elif fallback in (\"number_then_full\", \"number_only\"):\n",
    "            m = RE_NUMBER.findall(text)\n",
    "            if m:\n",
    "                # Use last number found\n",
    "                result = m[-1]\n",
    "            elif fallback == \"number_then_full\":\n",
    "                # Return full text if no number found\n",
    "                result = text\n",
    "    return result\n",
    "\n",
    "\n",
    "# LaTeX formatting patterns to normalize\n",
    "LATEX_FIXES = [\n",
    "    (r\"\\\\left\\s*\", \"\"),\n",
    "    (r\"\\\\right\\s*\", \"\"),\n",
    "    (r\"\\\\,|\\\\!|\\\\;|\\\\:\", \"\"),\n",
    "    (r\"\\\\cdot\", \"*\"),\n",
    "    (r\"\\u00B7|\\u00D7\", \"*\"),\n",
    "    (r\"\\\\\\^\\\\circ\", \"\"),\n",
    "    (r\"\\\\dfrac\", r\"\\\\frac\"),\n",
    "    (r\"\\\\tfrac\", r\"\\\\frac\"),\n",
    "    (r\"°\", \"\"),\n",
    "]\n",
    "\n",
    "# Regex to strip chat special tokens like <|assistant|>\n",
    "RE_SPECIAL = re.compile(r\"<\\|[^>]+?\\|>\")\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Normalize mathematical text by handling LaTeX, unicode, fractions, etc.\n",
    "    This is crucial for comparing model outputs with ground truth answers.\n",
    "    \n",
    "    Args:\n",
    "        text: Raw mathematical expression (may contain LaTeX)\n",
    "    \n",
    "    Returns:\n",
    "        Normalized lowercase string suitable for comparison\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    text = RE_SPECIAL.sub(\"\", text).strip()\n",
    "    \n",
    "    # Map for converting unicode superscripts to normal characters\n",
    "    SUPERSCRIPT_MAP = {\n",
    "        \"⁰\": \"0\", \"¹\": \"1\", \"²\": \"2\", \"³\": \"3\", \"⁴\": \"4\",\n",
    "        \"⁵\": \"5\", \"⁶\": \"6\", \"⁷\": \"7\", \"⁸\": \"8\", \"⁹\": \"9\",\n",
    "        \"⁺\": \"+\", \"⁻\": \"-\", \"⁽\": \"(\", \"⁾\": \")\",\n",
    "    }\n",
    "    \n",
    "    # Remove angle-degree markers\n",
    "    text = re.sub(r\"\\^\\s*\\{\\s*\\\\circ\\s*\\}\", \"\", text)   # ^{\\circ}\n",
    "    text = re.sub(r\"\\^\\s*\\\\circ\", \"\", text)             # ^\\circ\n",
    "    text = text.replace(\"°\", \"\")                        # Unicode degree\n",
    "\n",
    "    # Unwrap \\text{...} if the whole string is wrapped\n",
    "    match = re.match(r\"^\\\\text\\{(?P<x>.+?)\\}$\", text)\n",
    "    if match:\n",
    "        text = match.group(\"x\")\n",
    "\n",
    "    # Strip inline/display math wrappers \\( \\) \\[ \\]\n",
    "    text = re.sub(r\"\\\\\\(|\\\\\\)|\\\\\\[|\\\\\\]\", \"\", text)\n",
    "\n",
    "    # Apply LaTeX canonicalization\n",
    "    for pat, rep in LATEX_FIXES:\n",
    "        text = re.sub(pat, rep, text)\n",
    "\n",
    "    def convert_superscripts(s, base=None):\n",
    "        \"\"\"Convert unicode superscripts to exponent notation\"\"\"\n",
    "        converted = \"\".join(\n",
    "            SUPERSCRIPT_MAP[ch] if ch in SUPERSCRIPT_MAP else ch\n",
    "            for ch in s\n",
    "        )\n",
    "        if base is None:\n",
    "            return converted\n",
    "        return f\"{base}**{converted}\"\n",
    "\n",
    "    # Convert unicode superscripts into exponent form (e.g., 2² -> 2**2)\n",
    "    text = re.sub(\n",
    "        r\"([0-9A-Za-z\\)\\]\\}])([⁰¹²³⁴⁵⁶⁷⁸⁹⁺⁻]+)\",\n",
    "        lambda m: convert_superscripts(m.group(2), base=m.group(1)),\n",
    "        text,\n",
    "    )\n",
    "    text = convert_superscripts(text)\n",
    "    \n",
    "    # Handle percentages and dollar signs\n",
    "    text = text.replace(\"\\\\%\", \"%\").replace(\"$\", \"\").replace(\"%\", \"\")\n",
    "    \n",
    "    # Convert \\sqrt{...} to sqrt(...)\n",
    "    text = re.sub(\n",
    "        r\"\\\\sqrt\\s*\\{([^}]*)\\}\",\n",
    "        lambda match: f\"sqrt({match.group(1)})\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"\\\\sqrt\\s+([^\\\\\\s{}]+)\",\n",
    "        lambda match: f\"sqrt({match.group(1)})\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    # Convert \\frac{a}{b} to (a)/(b)\n",
    "    text = re.sub(\n",
    "        r\"\\\\frac\\s*\\{([^{}]+)\\}\\s*\\{([^{}]+)\\}\",\n",
    "        lambda match: f\"({match.group(1)})/({match.group(2)})\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"\\\\frac\\s+([^\\s{}]+)\\s+([^\\s{}]+)\",\n",
    "        lambda match: f\"({match.group(1)})/({match.group(2)})\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    # Convert exponents: ^ to **\n",
    "    text = text.replace(\"^\", \"**\")\n",
    "    \n",
    "    # Handle mixed numbers (e.g., \"2 1/2\" -> \"2+1/2\")\n",
    "    text = re.sub(\n",
    "        r\"(?<=\\d)\\s+(\\d+/\\d+)\",\n",
    "        lambda match: \"+\" + match.group(1),\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    # Remove thousand separators (e.g., 1,234 -> 1234)\n",
    "    text = re.sub(\n",
    "        r\"(?<=\\d),(?=\\d\\d\\d(\\D|$))\",\n",
    "        \"\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    # Remove remaining braces and convert to lowercase\n",
    "    return text.replace(\"{\", \"\").replace(\"}\", \"\").strip().lower()\n",
    "\n",
    "\n",
    "def sympy_parser(expr):\n",
    "    \"\"\"\n",
    "    Parse a mathematical expression using SymPy with appropriate transformations.\n",
    "    \n",
    "    Args:\n",
    "        expr: Normalized mathematical expression string\n",
    "    \n",
    "    Returns:\n",
    "        SymPy expression object or None if parsing fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return spp.parse_expr(\n",
    "            expr,\n",
    "            transformations=(\n",
    "                # Standard transformations like handling parentheses\n",
    "                *spp.standard_transformations,\n",
    "                # Allow omitted multiplication symbols (e.g., \"2x\" -> \"2*x\")\n",
    "                spp.implicit_multiplication_application,\n",
    "            ),\n",
    "            # Evaluate during parsing so simple constants simplify (e.g., 2+3 -> 5)\n",
    "            evaluate=True,\n",
    "        )\n",
    "    except (SympifyError, SyntaxError, TypeError, IndexError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def equality_check(expr_gtruth, expr_pred):\n",
    "    \"\"\"\n",
    "    Check if two mathematical expressions are equivalent.\n",
    "    Uses both string comparison and symbolic SymPy comparison.\n",
    "    \n",
    "    Args:\n",
    "        expr_gtruth: Ground truth expression (normalized)\n",
    "        expr_pred: Predicted expression (normalized)\n",
    "    \n",
    "    Returns:\n",
    "        True if expressions are mathematically equivalent\n",
    "    \"\"\"\n",
    "    # First, check exact string match\n",
    "    if expr_gtruth == expr_pred:\n",
    "        return True\n",
    "\n",
    "    # Parse both expressions into SymPy objects\n",
    "    gtruth, pred = sympy_parser(expr_gtruth), sympy_parser(expr_pred)\n",
    "\n",
    "    # If both parsed successfully, try symbolic comparison\n",
    "    if gtruth is not None and pred is not None:\n",
    "        try:\n",
    "            # If the difference simplifies to 0, they are equivalent\n",
    "            return simplify(gtruth - pred) == 0\n",
    "        except (SympifyError, TypeError):\n",
    "            pass\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def split_into_parts(text):\n",
    "    \"\"\"\n",
    "    Split tuple/list answers into individual parts for comparison.\n",
    "    Example: \"(1, 2, 3)\" -> [\"1\", \"2\", \"3\"]\n",
    "    \n",
    "    Args:\n",
    "        text: Answer text that may be a tuple or list\n",
    "    \n",
    "    Returns:\n",
    "        List of individual answer parts\n",
    "    \"\"\"\n",
    "    result = [text]\n",
    "\n",
    "    if text:\n",
    "        # Check if text looks like a tuple or list, e.g. \"(a, b)\" or \"[a, b]\"\n",
    "        if (\n",
    "            len(text) >= 2\n",
    "            and text[0] in \"([\" and text[-1] in \")]\"\n",
    "            and \",\" in text[1:-1]\n",
    "        ):\n",
    "            # Split on commas inside brackets and strip whitespace\n",
    "            items = [p.strip() for p in text[1:-1].split(\",\")]\n",
    "            if all(items):\n",
    "                result = items\n",
    "    else:\n",
    "        # If text is empty, return an empty list\n",
    "        result = []\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def grade_answer(pred_text, gt_text):\n",
    "    \"\"\"\n",
    "    Grade a predicted answer against ground truth.\n",
    "    Handles normalization, tuple/list splitting, and symbolic comparison.\n",
    "    \n",
    "    Args:\n",
    "        pred_text: Model's predicted answer\n",
    "        gt_text: Ground truth answer\n",
    "    \n",
    "    Returns:\n",
    "        True if answer is correct, False otherwise\n",
    "    \"\"\"\n",
    "    result = False  # Default outcome if checks fail\n",
    "\n",
    "    # Only continue if both inputs are non-empty strings\n",
    "    if pred_text is not None and gt_text is not None:\n",
    "        # Normalize and split both answers into comparable parts\n",
    "        gt_parts = split_into_parts(normalize_text(gt_text))\n",
    "        pred_parts = split_into_parts(normalize_text(pred_text))\n",
    "\n",
    "        # Ensure both sides have same number of valid parts\n",
    "        if (gt_parts and pred_parts and len(gt_parts) == len(pred_parts)):\n",
    "            # Check each part for mathematical equivalence\n",
    "            result = all(\n",
    "                equality_check(gt, pred)\n",
    "                for gt, pred in zip(gt_parts, pred_parts)\n",
    "            )\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def render_prompt(question):\n",
    "    \"\"\"\n",
    "    Build the prompt for the model following Raschka's format.\n",
    "    Instructs model to output answer in \\\\boxed{} format.\n",
    "    \n",
    "    Args:\n",
    "        question: The math problem to solve\n",
    "    \n",
    "    Returns:\n",
    "        Formatted prompt string\n",
    "    \"\"\"\n",
    "    template = (\n",
    "        \"You are a helpful math assistant.\\n\"\n",
    "        \"Answer the question and write the final result on a new line as:\\n\"\n",
    "        \"\\\\boxed{ANSWER}\\n\\n\"\n",
    "        f\"Question:\\n{question}\\n\\nAnswer:\"\n",
    "    )\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eta_progress_message(processed, total, start_time, show_eta=False, label=\"Progress\"):\n",
    "    \"\"\"\n",
    "    Calculate and format progress message with ETA.\n",
    "    \n",
    "    Args:\n",
    "        processed: Number of items completed\n",
    "        total: Total number of items\n",
    "        start_time: Start time of the process\n",
    "        show_eta: Whether to show estimated time remaining\n",
    "        label: Label for the progress message\n",
    "    \n",
    "    Returns:\n",
    "        Formatted progress string\n",
    "    \"\"\"\n",
    "    progress = f\"{label}: {processed}/{total}\"\n",
    "    if not show_eta or processed <= 0:\n",
    "        return progress\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    if elapsed <= 0:\n",
    "        return progress\n",
    "\n",
    "    remaining = max(total - processed, 0)\n",
    "\n",
    "    if processed:\n",
    "        avg_time = elapsed / processed\n",
    "        eta_seconds = avg_time * remaining\n",
    "    else:\n",
    "        eta_seconds = 0\n",
    "\n",
    "    eta_seconds = max(int(round(eta_seconds)), 0)\n",
    "    minutes, rem_seconds = divmod(eta_seconds, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    \n",
    "    if hours:\n",
    "        eta = f\"{hours}h {minutes:02d}m {rem_seconds:02d}s\"\n",
    "    elif minutes:\n",
    "        eta = f\"{minutes:02d}m {rem_seconds:02d}s\"\n",
    "    else:\n",
    "        eta = f\"{rem_seconds:02d}s\"\n",
    "\n",
    "    return f\"{progress} | ETA: {eta}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_batch(model, tokenizer, prompts, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    Generate responses for a batch of prompts.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model\n",
    "        tokenizer: The tokenizer\n",
    "        prompts: List of prompt strings\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "    \n",
    "    Returns:\n",
    "        List of generated text strings (without the input prompt)\n",
    "    \"\"\"\n",
    "    # Tokenize all prompts\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate responses\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Greedy decoding for reproducibility\n",
    "        )\n",
    "\n",
    "    # Extract only generated tokens (remove input prompt)\n",
    "    gen_only_ids = [\n",
    "        output_ids[len(input_ids):]\n",
    "        for input_ids, output_ids in zip(inputs[\"input_ids\"], generated)\n",
    "    ]\n",
    "\n",
    "    # Decode to text\n",
    "    outputs = tokenizer.batch_decode(gen_only_ids, skip_special_tokens=True)\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_math500(cfg: ReasoningConfig):\n",
    "    \"\"\"\n",
    "    Main evaluation function using Raschka's improved pipeline.\n",
    "    \n",
    "    Args:\n",
    "        cfg: Configuration object with model and dataset settings\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (num_correct, num_total, accuracy)\n",
    "    \"\"\"\n",
    "    # Load the MATH-500 benchmark dataset\n",
    "    dataset = load_dataset(cfg.dataset_name, split=cfg.dataset_split)\n",
    "\n",
    "    # Limit to subset if specified\n",
    "    if cfg.limit_eval_samples is not None:\n",
    "        dataset = dataset.select(range(cfg.limit_eval_samples))\n",
    "\n",
    "    num_examples = len(dataset)\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    print(f\"Loading model: {cfg.model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize counters\n",
    "    num_correct = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Open output file for saving detailed results\n",
    "    out_path = Path(cfg.output_file) if cfg.output_file else Path(\"math500_results_500.jsonl\")\n",
    "    \n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, num_examples, cfg.batch_size), desc=\"Evaluating\"):\n",
    "            batch_end = min(i + cfg.batch_size, num_examples)\n",
    "            batch_data = dataset[i:batch_end]\n",
    "            \n",
    "            # Handle both single items and batches\n",
    "            if isinstance(batch_data[\"problem\"], str):\n",
    "                batch_problems = [batch_data[\"problem\"]]\n",
    "                batch_answers = [batch_data[\"answer\"]]\n",
    "            else:\n",
    "                batch_problems = batch_data[\"problem\"]\n",
    "                batch_answers = batch_data[\"answer\"]\n",
    "            \n",
    "            # Build prompts\n",
    "            prompts = [render_prompt(q) for q in batch_problems]\n",
    "            \n",
    "            # Generate responses\n",
    "            generated_texts = generate_batch(model, tokenizer, prompts, cfg.max_new_tokens)\n",
    "            \n",
    "            # Grade each response\n",
    "            for idx, (problem, gt_answer, gen_text) in enumerate(zip(batch_problems, batch_answers, generated_texts)):\n",
    "                # Extract answer from generated text\n",
    "                extracted = extract_final_candidate(gen_text)\n",
    "                \n",
    "                # Grade the answer\n",
    "                is_correct = grade_answer(extracted, gt_answer)\n",
    "                num_correct += int(is_correct)\n",
    "                \n",
    "                # Save detailed record\n",
    "                record = {\n",
    "                    \"index\": i + idx + 1,\n",
    "                    \"problem\": problem,\n",
    "                    \"ground_truth\": gt_answer,\n",
    "                    \"generated_text\": gen_text,\n",
    "                    \"extracted_answer\": extracted,\n",
    "                    \"correct\": bool(is_correct),\n",
    "                }\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "            \n",
    "            # Print progress\n",
    "            processed = batch_end\n",
    "            progress_msg = eta_progress_message(\n",
    "                processed, num_examples, start_time, \n",
    "                show_eta=True, label=\"MATH-500\"\n",
    "            )\n",
    "            print(f\"\\r{progress_msg} | Correct: {num_correct}/{processed}\", end=\"\", flush=True)\n",
    "    \n",
    "    print()  # New line after progress\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    accuracy = num_correct / num_examples if num_examples else 0.0\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"MATH-500 Evaluation Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total examples:     {num_examples}\")\n",
    "    print(f\"Correct answers:    {num_correct}\")\n",
    "    print(f\"Accuracy:           {accuracy*100:.2f}% ({num_correct}/{num_examples})\")\n",
    "    print(f\"Total time:         {elapsed_time/60:.1f} minutes\")\n",
    "    print(f\"Results saved to:   {out_path}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return num_correct, num_examples, accuracy\n",
    "\n",
    "\n",
    "# Initialize configuration\n",
    "cfg_reasoning = ReasoningConfig(\n",
    "    limit_eval_samples=500,  # Evaluate on 50 samples for testing\n",
    "    batch_size=2,  # Process 2 at a time due to model size\n",
    "    max_new_tokens=512,  # Allow enough tokens for reasoning steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n",
      "  Dataset: HuggingFaceH4/MATH-500\n",
      "  Samples: 500\n",
      "  Batch size: 2\n",
      "  Max new tokens: 512\n",
      "  Output file: math500_results_500.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Display the configuration being used for evaluation\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {cfg_reasoning.model_name}\")\n",
    "print(f\"  Dataset: {cfg_reasoning.dataset_name}\")\n",
    "print(f\"  Samples: {cfg_reasoning.limit_eval_samples}\")\n",
    "print(f\"  Batch size: {cfg_reasoning.batch_size}\")\n",
    "print(f\"  Max new tokens: {cfg_reasoning.max_new_tokens}\")\n",
    "print(f\"  Output file: {cfg_reasoning.output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:53<00:00,  6.65s/it]\n",
      "Evaluating:   0%|          | 0/250 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATH-500: 2/500 | ETA: 2h 08m 01s | Correct: 1/2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 1/250 [00:30<2:08:00, 30.85s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATH-500: 4/500 | ETA: 2h 05m 27s | Correct: 3/4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 2/250 [01:00<2:05:05, 30.26s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATH-500: 6/500 | ETA: 1h 53m 23s | Correct: 4/6"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 3/250 [01:22<1:48:54, 26.45s/it]Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Run the improved evaluation with Raschka's pipeline\n",
    "# This uses:\n",
    "# - Proper LaTeX normalization (handles \\frac{}, \\sqrt{}, etc.)\n",
    "# - SymPy symbolic comparison (recognizes mathematically equivalent expressions)\n",
    "# - Robust answer extraction from \\boxed{} format with fallback to last number\n",
    "# - Detailed logging to JSONL file for error analysis\n",
    "\n",
    "num_correct, num_total, accuracy = evaluate_math500(cfg_reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Accuracy: 54.00%\n",
      "Correct: 27/50\n",
      "\n",
      "--- Sample Results from math500_results.jsonl ---\n",
      "\n",
      "Example 1:\n",
      "  Problem: Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the...\n",
      "  Ground Truth: \\left( 3, \\frac{\\pi}{2} \\right)\n",
      "  Extracted: (3, \\frac{\\pi}{2})\n",
      "  Correct: True\n",
      "\n",
      "Example 2:\n",
      "  Problem: Define\n",
      "\\[p = \\sum_{k = 1}^\\infty \\frac{1}{k^2} \\quad \\text{and} \\quad q = \\sum_{k = 1}^\\infty \\frac{...\n",
      "  Ground Truth: p - q\n",
      "  Extracted: \\frac{q}{2}\n",
      "  Correct: False\n",
      "\n",
      "Example 3:\n",
      "  Problem: If $f(x) = \\frac{3x-2}{x-2}$, what is the value of $f(-2) +f(-1)+f(0)$? Express your answer as a com...\n",
      "  Ground Truth: \\frac{14}{3}\n",
      "  Extracted: \\dfrac{14}{3}\n",
      "  Correct: True\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Display final results\n",
    "print(f\"\\nFinal Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Correct: {num_correct}/{num_total}\")\n",
    "\n",
    "# Example: Load and inspect some results\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "results_file = Path(cfg_reasoning.output_file)\n",
    "if results_file.exists():\n",
    "    print(f\"\\n--- Sample Results from {results_file} ---\")\n",
    "    with open(results_file, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 3:  # Show first 3 examples\n",
    "                break\n",
    "            record = json.loads(line)\n",
    "            print(f\"\\nExample {record['index']}:\")\n",
    "            print(f\"  Problem: {record['problem'][:100]}...\")\n",
    "            print(f\"  Ground Truth: {record['ground_truth']}\")\n",
    "            print(f\"  Extracted: {record['extracted_answer']}\")\n",
    "            print(f\"  Correct: {record['correct']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
