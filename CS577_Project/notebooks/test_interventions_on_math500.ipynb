{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Top Interventions on MATH-500 with Accuracy Measurement\n",
    "\n",
    "## Objective\n",
    "\n",
    "Test whether reasoning style changes from interventions translate to **correctness improvements**.\n",
    "\n",
    "## Baseline Performance\n",
    "\n",
    "- **RL Model (QwQ-32B)**: 31.80% (159/500)\n",
    "- **Distilled Model**: 43.40% (217/500)\n",
    "- **Gap to close**: 11.6 percentage points\n",
    "\n",
    "## Top Intervention Candidates\n",
    "\n",
    "Based on corrected quality analysis:\n",
    "\n",
    "1. **Layer 16, strength=-2.0**: Most thorough reasoning (28 steps, 4 backtracking)\n",
    "   - Hypothesis: Thoroughness catches more errors\n",
    "\n",
    "2. **Layer 0, strength=-1.5**: Move toward distilled model (which performs better)\n",
    "   - Hypothesis: Steering toward better performer improves accuracy\n",
    "\n",
    "3. **Layer 20, strength=-1.0**: Most concise reasoning (11 steps, 1 backtracking)\n",
    "   - Hypothesis: Efficiency avoids over-thinking errors\n",
    "\n",
    "4. **Layer 0, strength=-2.0**: Stronger move toward distilled (13 steps, complete)\n",
    "   - Alternative to test strength effect\n",
    "\n",
    "5. **Layer 16, strength=-0.5**: Moderate reasoning (12 steps, complete)\n",
    "   - Control: non-truncated result at Layer 16\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "- **Minimal**: >32.80% (+1pp, 10% of gap closed)\n",
    "- **Moderate**: 34.80-36.80% (+3-5pp, 25-40% of gap)\n",
    "- **Strong**: 39.80-41.80% (+8-10pp, 70-85% of gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.append('../pipeline')\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport json\nimport re\nfrom pathlib import Path\nimport numpy as np\n\n# Import intervention and evaluator\nfrom intervention import ActivationPatcher\nfrom evaluator import ReasoningEvaluator\n\nprint(\"✓ Imports successful\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "RL_MODEL = \"Qwen/QwQ-32B\"\n",
    "DATASET = \"HuggingFaceH4/MATH-500\"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(\"/scratch/gilbreth/sramishe/results_QwQ_R1/interventions_math500\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Intervention configurations to test\n",
    "INTERVENTIONS = [\n",
    "    {\"name\": \"baseline\", \"layer\": None, \"strength\": 0.0},\n",
    "    {\"name\": \"L16_s-2.0_thorough\", \"layer\": 16, \"strength\": -2.0},\n",
    "    {\"name\": \"L0_s-1.5_distilled\", \"layer\": 0, \"strength\": -1.5},\n",
    "    {\"name\": \"L20_s-1.0_concise\", \"layer\": 20, \"strength\": -1.0},\n",
    "    {\"name\": \"L0_s-2.0_distilled_strong\", \"layer\": 0, \"strength\": -2.0},\n",
    "    {\"name\": \"L16_s-0.5_moderate\", \"layer\": 16, \"strength\": -0.5},\n",
    "]\n",
    "\n",
    "# Generation parameters\n",
    "MAX_NEW_TOKENS = 2048  # Increased to avoid truncation\n",
    "TEMPERATURE = 0.0  # Greedy decoding for reproducibility\n",
    "\n",
    "print(f\"Testing {len(INTERVENTIONS)} intervention configurations\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MATH-500\n",
    "print(\"Loading MATH-500 dataset...\")\n",
    "dataset = load_dataset(DATASET, split=\"test\")\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} problems\")\n",
    "print(f\"Sample problem:\")\n",
    "print(f\"  Question: {dataset[0]['problem'][:100]}...\")\n",
    "print(f\"  Answer: {dataset[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/scratch/gilbreth/sramishe'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/scratch/gilbreth/sramishe/transformers'\n",
    "\n",
    "print(\"Loading RL model...\")\n",
    "rl_model = AutoModelForCausalLM.from_pretrained(\n",
    "    RL_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(RL_MODEL)\n",
    "\n",
    "print(f\"✓ Model loaded: {RL_MODEL}\")\n",
    "print(f\"  Layers: {rl_model.config.num_hidden_layers}\")\n",
    "print(f\"  Hidden size: {rl_model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reasoning directions\n",
    "print(\"Loading reasoning directions...\")\n",
    "directions_path = Path(\"/scratch/gilbreth/sramishe/results_QwQ_R1/results_improved/reasoning_directions.pt\")\n",
    "\n",
    "if directions_path.exists():\n",
    "    directions = torch.load(directions_path)\n",
    "    print(f\"✓ Loaded directions for {len(directions)} layers\")\n",
    "    print(f\"  Available layers: {sorted(directions.keys())}\")\n",
    "else:\n",
    "    print(\"⚠️  Directions file not found!\")\n",
    "    print(\"   Need to run direction extraction first\")\n",
    "    raise FileNotFoundError(f\"Directions not found at {directions_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Extraction and Grading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import simplify, sympify, N\n",
    "from sympy.parsing.latex import parse_latex\n",
    "\n",
    "def extract_answer(text):\n",
    "    \"\"\"\n",
    "    Extract answer from model output.\n",
    "    Looks for \\boxed{} format first, then falls back to last number.\n",
    "    \"\"\"\n",
    "    # Look for \\boxed{...}\n",
    "    boxed_pattern = r'\\\\boxed\\{([^}]+)\\}'\n",
    "    boxed_matches = re.findall(boxed_pattern, text)\n",
    "    if boxed_matches:\n",
    "        return boxed_matches[-1].strip()\n",
    "    \n",
    "    # Fallback: find last number-like pattern\n",
    "    number_pattern = r'[-+]?\\d*\\.?\\d+'\n",
    "    number_matches = re.findall(number_pattern, text)\n",
    "    if number_matches:\n",
    "        return number_matches[-1].strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "def normalize_answer(answer):\n",
    "    \"\"\"\n",
    "    Normalize answer for comparison.\n",
    "    Handles LaTeX, fractions, etc.\n",
    "    \"\"\"\n",
    "    if answer is None:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Try parsing as LaTeX first\n",
    "        if '\\\\' in answer:\n",
    "            expr = parse_latex(answer)\n",
    "        else:\n",
    "            expr = sympify(answer)\n",
    "        \n",
    "        # Simplify\n",
    "        simplified = simplify(expr)\n",
    "        return simplified\n",
    "    except:\n",
    "        # If parsing fails, return string\n",
    "        return answer.strip().lower()\n",
    "\n",
    "def grade_answer(predicted, ground_truth):\n",
    "    \"\"\"\n",
    "    Grade predicted answer against ground truth.\n",
    "    Returns True if correct, False otherwise.\n",
    "    \"\"\"\n",
    "    pred_norm = normalize_answer(predicted)\n",
    "    truth_norm = normalize_answer(ground_truth)\n",
    "    \n",
    "    if pred_norm is None or truth_norm is None:\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Try symbolic comparison\n",
    "        return simplify(pred_norm - truth_norm) == 0\n",
    "    except:\n",
    "        # Fallback to string comparison\n",
    "        return str(pred_norm) == str(truth_norm)\n",
    "\n",
    "print(\"✓ Grading functions defined\")\n",
    "\n",
    "# Test grading\n",
    "test_cases = [\n",
    "    (\"42\", \"42\", True),\n",
    "    (\"1/2\", \"0.5\", True),\n",
    "    (\"\\\\frac{1}{2}\", \"0.5\", True),\n",
    "    (\"42\", \"43\", False),\n",
    "]\n",
    "\n",
    "print(\"\\nTesting grading function:\")\n",
    "for pred, truth, expected in test_cases:\n",
    "    result = grade_answer(pred, truth)\n",
    "    status = \"✓\" if result == expected else \"✗\"\n",
    "    print(f\"  {status} grade_answer('{pred}', '{truth}') = {result} (expected {expected})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_intervention(intervention_config, dataset, model, tokenizer, directions, evaluator):\n    \"\"\"\n    Evaluate a single intervention configuration on MATH-500.\n    \n    Returns:\n        dict with accuracy, results, and metadata\n    \"\"\"\n    name = intervention_config['name']\n    layer = intervention_config['layer']\n    strength = intervention_config['strength']\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"Evaluating: {name}\")\n    if layer is not None:\n        print(f\"  Layer: {layer}, Strength: {strength}\")\n    else:\n        print(f\"  Baseline (no intervention)\")\n    print(f\"{'='*80}\")\n    \n    # Initialize intervention\n    if layer is not None:\n        patcher = ActivationPatcher(\n            model=model,\n            directions={layer: directions[layer]}\n        )\n    else:\n        patcher = None\n    \n    results = []\n    correct = 0\n    total = 0\n    \n    for i, sample in enumerate(tqdm(dataset, desc=f\"Evaluating {name}\")):\n        problem = sample['problem']\n        ground_truth = sample['answer']\n        \n        # Format prompt\n        prompt = f\"\"\"user: {problem}\nassistant:\"\"\"\n        \n        # Generate with intervention\n        if patcher is not None:\n            output = patcher.generate_with_intervention(\n                tokenizer=tokenizer,\n                prompt=prompt,\n                layers=[layer],  # layers is a list\n                strength=strength,\n                max_new_tokens=MAX_NEW_TOKENS,\n                do_sample=False,\n                pad_token_id=tokenizer.eos_token_id\n            )\n            # Remove prompt from output\n            output = output[len(prompt):].strip()\n        else:\n            # Baseline generation\n            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=MAX_NEW_TOKENS,\n                do_sample=False,\n                pad_token_id=tokenizer.eos_token_id\n            )\n            output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n            output = output[len(prompt):].strip()\n        \n        # Extract answer\n        predicted_answer = extract_answer(output)\n        \n        # Grade\n        is_correct = grade_answer(predicted_answer, ground_truth)\n        \n        if is_correct:\n            correct += 1\n        total += 1\n        \n        # Get quality metrics\n        tokens = evaluator.count_tokens(output, tokenizer, split_by_tags=True)\n        quality = evaluator.analyze_reasoning_quality(output)\n        \n        # Store result\n        result = {\n            'problem_id': i,\n            'problem': problem,\n            'ground_truth': ground_truth,\n            'output': output,\n            'predicted_answer': predicted_answer,\n            'is_correct': is_correct,\n            'tokens': tokens,\n            'quality': quality\n        }\n        results.append(result)\n        \n        # Print progress every 50 problems\n        if (i + 1) % 50 == 0:\n            current_acc = 100.0 * correct / total\n            print(f\"  Progress: {i+1}/{len(dataset)}, Accuracy: {current_acc:.2f}% ({correct}/{total})\")\n    \n    # Calculate final accuracy\n    accuracy = 100.0 * correct / total\n    \n    print(f\"\\n{'='*80}\")\n    print(f\"RESULTS: {name}\")\n    print(f\"  Correct: {correct}/{total}\")\n    print(f\"  Accuracy: {accuracy:.2f}%\")\n    print(f\"  Baseline: 31.80%\")\n    print(f\"  Improvement: {accuracy - 31.80:+.2f}pp\")\n    print(f\"{'='*80}\")\n    \n    return {\n        'config': intervention_config,\n        'accuracy': accuracy,\n        'correct': correct,\n        'total': total,\n        'results': results\n    }\n\nprint(\"✓ Evaluation function defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ReasoningEvaluator()\n",
    "print(\"✓ Evaluator initialized (with fixed extract_think_tags)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluations\n",
    "\n",
    "**Note:** This will take ~6-8 hours total (1+ hour per intervention configuration).\n",
    "\n",
    "To save time, you can:\n",
    "1. Test on a subset first (e.g., first 50 problems)\n",
    "2. Run interventions sequentially or in parallel on multiple GPUs\n",
    "3. Save checkpoints after each intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Quick test on subset (recommended first)\n",
    "TEST_SUBSET = True\n",
    "SUBSET_SIZE = 50  # Test on first 50 problems\n",
    "\n",
    "if TEST_SUBSET:\n",
    "    print(f\"\\n⚠️  TESTING ON SUBSET: First {SUBSET_SIZE} problems\")\n",
    "    print(f\"   Change TEST_SUBSET=False to run on full dataset\\n\")\n",
    "    test_dataset = dataset.select(range(SUBSET_SIZE))\n",
    "else:\n",
    "    print(\"\\n✓ Running on FULL DATASET (500 problems)\\n\")\n",
    "    test_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all evaluations\n",
    "all_results = {}\n",
    "\n",
    "for intervention in INTERVENTIONS:\n",
    "    name = intervention['name']\n",
    "    \n",
    "    # Check if already computed\n",
    "    result_file = OUTPUT_DIR / f\"{name}_results.json\"\n",
    "    if result_file.exists():\n",
    "        print(f\"\\n⚠️  Skipping {name} - results already exist at {result_file}\")\n",
    "        print(f\"   Delete the file to re-run\\n\")\n",
    "        with open(result_file, 'r') as f:\n",
    "            all_results[name] = json.load(f)\n",
    "        continue\n",
    "    \n",
    "    # Run evaluation\n",
    "    result = evaluate_intervention(\n",
    "        intervention_config=intervention,\n",
    "        dataset=test_dataset,\n",
    "        model=rl_model,\n",
    "        tokenizer=tokenizer,\n",
    "        directions=directions,\n",
    "        evaluator=evaluator\n",
    "    )\n",
    "    \n",
    "    # Save individual result\n",
    "    with open(result_file, 'w') as f:\n",
    "        json.dump(result, f, indent=2, default=str)\n",
    "    print(f\"\\n✓ Saved results to {result_file}\")\n",
    "    \n",
    "    all_results[name] = result\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL EVALUATIONS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "for name, result in all_results.items():\n",
    "    config = result['config']\n",
    "    summary_data.append({\n",
    "        'Name': name,\n",
    "        'Layer': config['layer'] if config['layer'] is not None else 'N/A',\n",
    "        'Strength': config['strength'],\n",
    "        'Accuracy (%)': result['accuracy'],\n",
    "        'Correct': result['correct'],\n",
    "        'Total': result['total'],\n",
    "        'Improvement (pp)': result['accuracy'] - 31.80\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.sort_values('Accuracy (%)', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: INTERVENTION ACCURACY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"\\nBaseline (RL model, no intervention): 31.80%\")\n",
    "print(\"Target (Distilled model): 43.40%\")\n",
    "print(\"Gap to close: 11.6pp\")\n",
    "\n",
    "# Save summary\n",
    "summary_file = OUTPUT_DIR / \"summary.csv\"\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "print(f\"\\n✓ Summary saved to {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis: Best Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best intervention\n",
    "best_name = summary_df.iloc[0]['Name']\n",
    "best_result = all_results[best_name]\n",
    "best_accuracy = best_result['accuracy']\n",
    "improvement = best_accuracy - 31.80\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"BEST INTERVENTION: {best_name}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Accuracy: {best_accuracy:.2f}%\")\n",
    "print(f\"Improvement: {improvement:+.2f}pp\")\n",
    "print(f\"Gap closed: {100*improvement/11.6:.1f}% of 11.6pp gap\")\n",
    "\n",
    "if improvement > 1.0:\n",
    "    print(\"\\n✅ SUCCESS: Achieved >1pp improvement (minimal goal)\")\n",
    "    if improvement > 3.0:\n",
    "        print(\"✅ MODERATE SUCCESS: Achieved >3pp improvement\")\n",
    "    if improvement > 8.0:\n",
    "        print(\"✅ STRONG SUCCESS: Achieved >8pp improvement\")\n",
    "elif improvement > 0:\n",
    "    print(\"\\n⚠️  Marginal improvement, but not statistically significant\")\n",
    "else:\n",
    "    print(\"\\n❌ No improvement over baseline\")\n",
    "\n",
    "# Quality metrics comparison\n",
    "best_results = best_result['results']\n",
    "correct_results = [r for r in best_results if r['is_correct']]\n",
    "incorrect_results = [r for r in best_results if not r['is_correct']]\n",
    "\n",
    "if len(correct_results) > 0 and len(incorrect_results) > 0:\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"QUALITY METRICS: Correct vs Incorrect\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Average reasoning steps\n",
    "    correct_steps = np.mean([r['quality']['reasoning_steps'] for r in correct_results])\n",
    "    incorrect_steps = np.mean([r['quality']['reasoning_steps'] for r in incorrect_results])\n",
    "    print(f\"Average reasoning steps:\")\n",
    "    print(f\"  Correct: {correct_steps:.1f}\")\n",
    "    print(f\"  Incorrect: {incorrect_steps:.1f}\")\n",
    "    \n",
    "    # Average think tokens\n",
    "    correct_tokens = np.mean([r['tokens']['think_tokens'] for r in correct_results])\n",
    "    incorrect_tokens = np.mean([r['tokens']['think_tokens'] for r in incorrect_results])\n",
    "    print(f\"\\nAverage think tokens:\")\n",
    "    print(f\"  Correct: {correct_tokens:.1f}\")\n",
    "    print(f\"  Incorrect: {incorrect_tokens:.1f}\")\n",
    "    \n",
    "    # Backtracking\n",
    "    correct_backtrack = np.mean([r['quality']['backtracking_count'] for r in correct_results])\n",
    "    incorrect_backtrack = np.mean([r['quality']['backtracking_count'] for r in incorrect_results])\n",
    "    print(f\"\\nAverage backtracking:\")\n",
    "    print(f\"  Correct: {correct_backtrack:.1f}\")\n",
    "    print(f\"  Incorrect: {incorrect_backtrack:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Outputs: Correct vs Incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample correct answer\n",
    "if len(correct_results) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE CORRECT ANSWER\")\n",
    "    print(\"=\"*80)\n",
    "    sample = correct_results[0]\n",
    "    print(f\"Problem: {sample['problem'][:200]}...\")\n",
    "    print(f\"\\nGround Truth: {sample['ground_truth']}\")\n",
    "    print(f\"Predicted: {sample['predicted_answer']}\")\n",
    "    print(f\"\\nReasoning steps: {sample['quality']['reasoning_steps']}\")\n",
    "    print(f\"Think tokens: {sample['tokens']['think_tokens']}\")\n",
    "    print(f\"\\nOutput (first 500 chars):\\n{sample['output'][:500]}...\")\n",
    "\n",
    "# Show sample incorrect answer\n",
    "if len(incorrect_results) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE INCORRECT ANSWER\")\n",
    "    print(\"=\"*80)\n",
    "    sample = incorrect_results[0]\n",
    "    print(f\"Problem: {sample['problem'][:200]}...\")\n",
    "    print(f\"\\nGround Truth: {sample['ground_truth']}\")\n",
    "    print(f\"Predicted: {sample['predicted_answer']}\")\n",
    "    print(f\"\\nReasoning steps: {sample['quality']['reasoning_steps']}\")\n",
    "    print(f\"Think tokens: {sample['tokens']['think_tokens']}\")\n",
    "    print(f\"\\nOutput (first 500 chars):\\n{sample['output'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate report\n",
    "report = f\"\"\"# MATH-500 Intervention Accuracy Results\n",
    "\n",
    "## Summary\n",
    "\n",
    "Dataset: {'SUBSET (first ' + str(SUBSET_SIZE) + ' problems)' if TEST_SUBSET else 'FULL MATH-500 (500 problems)'}\n",
    "\n",
    "### Baseline Performance\n",
    "- RL Model (no intervention): 31.80%\n",
    "- Distilled Model: 43.40%\n",
    "- Gap: 11.6pp\n",
    "\n",
    "### Intervention Results\n",
    "\n",
    "{summary_df.to_markdown(index=False)}\n",
    "\n",
    "### Best Intervention\n",
    "\n",
    "**{best_name}**\n",
    "- Accuracy: {best_accuracy:.2f}%\n",
    "- Improvement: {improvement:+.2f}pp\n",
    "- Gap closed: {100*improvement/11.6:.1f}%\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if improvement > 1.0:\n",
    "    report += f\"✅ **SUCCESS**: Intervention improved accuracy by {improvement:.2f}pp\\n\\n\"\n",
    "    report += \"This demonstrates that activation steering can improve reasoning performance.\\n\\n\"\n",
    "    \n",
    "    if improvement > 3.0:\n",
    "        report += \"The improvement is substantial (>3pp), closing 25%+ of the gap.\\n\"\n",
    "else:\n",
    "    report += f\"❌ **NO SIGNIFICANT IMPROVEMENT**: Intervention changed accuracy by {improvement:+.2f}pp\\n\\n\"\n",
    "    report += \"Reasoning style changes (verbosity, thoroughness) did not translate to correctness.\\n\\n\"\n",
    "\n",
    "# Save report\n",
    "report_file = OUTPUT_DIR / \"REPORT.md\"\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REPORT SAVED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Location: {report_file}\")\n",
    "print(\"\\nAll results saved to:\")\n",
    "print(f\"  - Individual results: {OUTPUT_DIR}/*_results.json\")\n",
    "print(f\"  - Summary CSV: {OUTPUT_DIR}/summary.csv\")\n",
    "print(f\"  - Report: {OUTPUT_DIR}/REPORT.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}